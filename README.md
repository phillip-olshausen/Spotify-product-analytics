[README.md](https://github.com/user-attachments/files/25169956/README.md)
# Spotify-Style Product Analytics (SQL-first) — Activation, Engagement & Retention

This project builds **Spotify-like product analytics** metrics on a **real listening-events dataset** (Last.fm 1K listening history) using **PostgreSQL + SQL**.  
Python is used only for **data acquisition + preparation** (export to CSV). The **analysis itself is SQL-first**.



## What you’ll compute (Spotify-style)
- **DAU / WAU / MAU**
- **Activation rate (24h)** (proxy: signup_date if available, else first-seen date)
- **Retention** (D1 / D7 / D30)
- **Sessionization** (30-minute inactivity rule)
- **“Save” proxy** (tracks listened ≥ N times)

## Tech
- PostgreSQL (queries run in DBeaver or psql)
- Python (datasets → CSV)
- Dataset: Last.fm 1K listening events via Hugging Face Datasets

## Repo structure
```
spotify_product_analytics_sql/
├── python/
│   ├── 01_download_and_export.py
│   └── 02_prepare_tables.py
├── sql/
│   ├── 01_schema.sql
│   ├── 02_build_sessions.sql
│   ├── 03_build_saves_proxy.sql
│   └── queries/
│       ├── 10_dau.sql
│       ├── 11_wau.sql
│       ├── 12_mau.sql
│       ├── 20_activation_24h.sql
│       ├── 30_retention_d1_d7_d30.sql
│       ├── 40_sessions_per_active_user.sql
│       └── 50_save_rate_proxy.sql
├── exports/        # optional: CSV exports from DBeaver
├── data/           # generated by Python scripts (large CSVs; not committed)
├── requirements.txt
└── .gitignore
```

## Quickstart

### 1) Create a virtual environment
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2) Download & export the raw dataset to CSV
```bash
python python/01_download_and_export.py
```
This writes:
- `data/events_train.csv`
- `data/events_valid.csv`
- `data/events_test.csv`

### 3) Prepare analytics tables (users + plays)
```bash
python python/02_prepare_tables.py
```
This writes:
- `data/users.csv`
- `data/plays.csv`

### 4) Load into PostgreSQL (recommended via DBeaver)
1. Create a database: `spotify_analytics`
2. Run `sql/01_schema.sql`
3. Import:
   - `data/users.csv` → table `users`
   - `data/plays.csv` → table `plays`
   
**DBeaver import path:** right-click table → **Import Data** → CSV → select file.

### 5) Build derived tables
Run:
- `sql/02_build_sessions.sql`
- `sql/03_build_saves_proxy.sql` (default listen threshold = 5)

### 6) Run metrics queries
Run the files under `sql/queries/`.

## Notes on “Spotify-like” proxies
The dataset does not contain explicit **skip** or **save** events.
- **Sessions** are inferred via time gaps (30 minutes)
- **Saves** are approximated by repeated listens (≥ N plays per user-track)

These are realistic product-analytics approaches when raw events don’t contain direct signals.

## Suggested portfolio bullets
- Built a SQL-first product analytics project (PostgreSQL) on real listening-event data to compute DAU/WAU/MAU, activation, retention cohorts, and engagement depth metrics.
- Implemented sessionization and “save” proxies to mirror Spotify-style behavioral signals when explicit events were unavailable.

## License / data
This repository stores only **code**. The generated CSVs in `data/` are ignored by git.
